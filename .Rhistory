}
classify_and_accuracy(pred, y_test, 0.5)
#best lambda is 0.03
#accuracy is 0.70
#b)
#ridge penalised model
logmodel_ridge = cv.glmnet(x_train, y_train, family = "binomial", alpha=1, lambda = grid, nfolds = k)
summary(logmodel_ridge)
#lambda that gives lowest cv error
best_lambda2 = logmodel_ridge$lambda.min
best_lambda2
#make predictions on test data with optimal 'lambda'
pred2 = predict(logmodel_ridge, newx = x_test, s=best_lambda2, type = 'response')
pred2
#compute accuracy with cutoff: 0.5
classify_and_accuracy(pred2, y_test, 0.5)
#best lambda is 0.01
#accuracy is 0.71
#c) SVM with linear kernel
#Convert label to factor for classification machine
credit_dataset$PROFITABLE = as.factor(credit_dataset$PROFITABLE)
#set seed and perform train / test  split
set.seed(12345)
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
credit_test = credit_dataset[test_inst,]
credit_train = credit_dataset[-test_inst,]
#build svm model with default settings
logmodel_svm = svm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, kernel ='linear', cost=1, cross=k, probability = TRUE)
summary(logmodel_svm)
#make predictions on test data
pred3 = predict(logmodel_svm, credit_test, probability = TRUE)
pred3
pred3_prob = attr(pred3, "probabilities")[,1] #to get the probabilities of positive class -'1'
pred3_prob
#compute accuracy with cutoff: 0.5
classify_and_accuracy(pred3_prob, credit_test$PROFITABLE, 0.5)
install.packages(c("gbm", "ISLR", "randomForest"))
library(e1071)
library(ROCR)
library(glmnet)
#load data
setwd("C:\\Users\\vivek_000\\Desktop\\Data Mining\\Assignment-3")
credit_dataset <-read.csv("Credit_Dataset.csv")
#create labels
credit_dataset$PROFITABLE = ifelse(credit_dataset$PROFIT>=0,1,0)
#convert required variables into factors
credit_dataset$CHK_ACCT = as.factor(credit_dataset$CHK_ACCT)
credit_dataset$SAV_ACCT = as.factor(credit_dataset$SAV_ACCT)
credit_dataset$HISTORY = as.factor(credit_dataset$HISTORY)
credit_dataset$JOB = as.factor(credit_dataset$JOB)
credit_dataset$TYPE = as.factor(credit_dataset$TYPE)
#set seed and perform train / test  split
set.seed(12345)
#convert to matrix for use with glmnet
credit_dataset_x = model.matrix(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,credit_dataset)
#credit_dataset_new = credit_dataset[c(2,6,12,17,18,3,4,7,10,20,24)]
#credit_dataset_x = model.matrix(PROFITABLE~.,credit_dataset_new)
credit_dataset_y = credit_dataset$PROFITABLE
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
x_test = credit_dataset_x[test_inst,]
x_train = credit_dataset_x[-test_inst,]
y_test = credit_dataset_y[test_inst]
y_train = credit_dataset_y[-test_inst]
#build the model on train data using CV approach
#set k=5 for cross validation
k<-5
#set grid search values
grid <- 10^seq(10,-2,length=100) #100 values between 10 to -2
#a)
#lasso penalised model
logmodel_lasso = cv.glmnet(x_train, y_train, family = "binomial", alpha=0, lambda = grid, nfolds = k)
summary(logmodel_lasso)
#lambda that gives lowest cv error
best_lambda = logmodel_lasso$lambda.min
best_lambda
#make predictions on test data with optimal 'lambda'
pred = predict(logmodel_lasso, newx = x_test, s=best_lambda, type = 'response')
pred
#compute accuracy with cutoff: 0.5
#create custom function
classify_and_accuracy <- function(probs, actuals, cutoff)
{
classifications <- ifelse(probs>cutoff,1,0)
accuracy <- sum(ifelse(classifications==actuals,1,0))/length(actuals)
return(accuracy)
}
classify_and_accuracy(pred, y_test, 0.5)
#best lambda is 0.03
#accuracy is 0.70
#b)
#ridge penalised model
logmodel_ridge = cv.glmnet(x_train, y_train, family = "binomial", alpha=1, lambda = grid, nfolds = k)
summary(logmodel_ridge)
#lambda that gives lowest cv error
best_lambda2 = logmodel_ridge$lambda.min
best_lambda2
#make predictions on test data with optimal 'lambda'
pred2 = predict(logmodel_ridge, newx = x_test, s=best_lambda2, type = 'response')
pred2
#compute accuracy with cutoff: 0.5
classify_and_accuracy(pred2, y_test, 0.5)
#best lambda is 0.01
#accuracy is 0.71
#c) SVM with linear kernel
#Convert label to factor for classification machine
credit_dataset$PROFITABLE = as.factor(credit_dataset$PROFITABLE)
#set seed and perform train / test  split
set.seed(12345)
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
credit_test = credit_dataset[test_inst,]
credit_train = credit_dataset[-test_inst,]
#build svm model with default settings
logmodel_svm = svm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, kernel ='linear', cost=1, cross=k, probability = TRUE)
summary(logmodel_svm)
#make predictions on test data
pred3 = predict(logmodel_svm, credit_test, probability = TRUE)
pred3
pred3_prob = attr(pred3, "probabilities")[,1] #to get the probabilities of positive class -'1'
pred3_prob
#compute accuracy with cutoff: 0.5
classify_and_accuracy(pred3_prob, credit_test$PROFITABLE, 0.5)
#accuracy is 0.70
#d)
#test
pred4 = predict(logmodel_svm, credit_test)
pred4
cm=table(credit_test$PROFITABLE, pred4)
cm
31+180
36+53
211/300
#you might need to do install.packages("randomForest"), install.packages("gbm"), and install.packages("ISLR")
library(randomForest)
library(gbm)
#note: the ISLR library goes along with the textbook. It comes with a bunch of datasets.
library(ISLR)
set.seed(1)
#we're going to use the OJ dataset (predicting whether a customer prefers Citrus Hill or Minute Maid orange juice)
summary(OJ)
train <- sample(nrow(OJ), .7*nrow(OJ))
test_size <- nrow(OJ)-length(train)
train
#we're going to use the OJ dataset (predicting whether a customer prefers Citrus Hill or Minute Maid orange juice)
OJ.head()
#we're going to use the OJ dataset (predicting whether a customer prefers Citrus Hill or Minute Maid orange juice)
head(OJ)
test_size
dim(OJ)
ls(OJ)
#bagging is just a random forest where m=p
#set mtry=17 (the number of features in the OJ dataset)
dim(OJ)
ls(OJ)
?randomForest()
bag.mod <- randomForest(Purchase~.,data=OJ,subset=train,mtry=17,importance=TRUE) #defaults to 500 bags
bag_preds <- predict(bag.mod,newdata=OJ[-train,])
bag.mod
summary(bag.mod)
bag.mod
bag_acc
bag_acc <- sum(ifelse(bag_preds==OJ$Purchase[-train],1,0))/test_size
bag_acc
class(OJ$Purchase)
##random forest example
#set mtry ~= sqrt(17) = 4
#set 1000 trees (this is something you can tune)
rf.mod <- randomForest(Purchase~.,data=OJ,subset=train,mtry=4,ntree=1000,importance=TRUE)
rf.mod
rf_preds <- predict(rf.mod,newdata=OJ[-train,])
rf_acc <- sum(ifelse(rf_preds==OJ$Purchase[-train],1,0))/test_size
rf_acc
#plot the variable importances (the average decrease in impurity when splitting across that variable)
importance(rf.mod)
varImpPlot(rf.mod)
varImpPlot(rf.mod)
par("mar")
par(mar=c(1,1,1,1))
varImpPlot(rf.mod)
par("mar")
varImpPlot(rf.mod)
varImpPlot(rf.mod)
##boosting example
boost_data <- OJ
class(boost_data$OJ)
class(boost_data$Purchase)
#needs a numerical target variable
boost_data$Purchase <- ifelse(OJ$Purchase=="CH",1,0)
class(boost_data$Purchase)
?svm
?gbm
#interaction.depth refers to the maximum depth of tree allowed
boost.mod <- gbm(Purchase~.,data=boost_data[train,],distribution="bernoulli",n.trees=1000,interaction.depth=4)
boost.mod
boost_preds <- predict(boost.mod,newdata=boost_data[-train,],type='response',n.trees=1000)
boost_preds
#classify with a cutoff and compute accuracy
boost_class <- ifelse(boost_preds>.5,1,0)
boost_acc <- sum(ifelse(boost_class==boost_data$Purchase[-train],1,0))/test_size
boost_acc
rf_acc
bag_acc
library(randomForest)
library(gbm)
#set seed
set.seed(12345)
#load data
setwd("C:\\Users\\vivek_000\\Desktop\\Data Mining\\Assignment-3")
credit_dataset <-read.csv("Credit_Dataset.csv")
credit_dataset$PROFITABLE = ifelse(credit_dataset$PROFIT>=0,1,0)
#Convert label to factor for classification using bagging & random forest
credit_dataset$PROFITABLE = as.factor(credit_dataset$PROFITABLE)
credit_dataset$CHK_ACCT = as.factor(credit_dataset$CHK_ACCT)
credit_dataset$SAV_ACCT = as.factor(credit_dataset$SAV_ACCT)
credit_dataset$HISTORY = as.factor(credit_dataset$HISTORY)
credit_dataset$JOB = as.factor(credit_dataset$JOB)
credit_dataset$TYPE = as.factor(credit_dataset$TYPE)
#split test/ rest
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
credit_test = credit_dataset[test_inst,]
credit_rest = credit_dataset[-test_inst,]
#split valid/ train
valid_inst = sample(nrow(credit_rest),0.25*nrow(credit_rest))
credit_valid = credit_rest[valid_inst,]
credit_train = credit_rest[-valid_inst,]
-valid_inst
valid_inst
credit_train
index(credit_train)
train_inst = sample(nrow(credit_train),nrow(credit_train))
train_inst
length(train_inst)
?randomForest
bag_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =[train_inst],mtry=10,importance=TRUE)
bag_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=10,importance=TRUE)
bag_model
pred1 = predict(bag_model, newdata = credit_valid)
pred1
valid_len = length(credit_valid)
valid_len
valid_len = nrow(credit_valid)
valid_len
bag_acc <- sum(ifelse(pred1==credit_valid$PROFITABLE,1,0))/valid_len
bag_acc
rf_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=3,importance=TRUE)
rf_model
pred2 = predict(rf_model, newdata = credit_valid)
pred2
#compute valid accuracy
rf_acc <- sum(ifelse(pred2==credit_valid$PROFITABLE,1,0))/valid_len
rf_acc
#Convert label to numeric for classification using bagging & random forest
credit_train$PROFITABLE = as.numeric(credit_train$PROFITABLE)
credit_valid$PROFITABLE = as.numeric(credit_valid$PROFITABLE)
?gbm
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
#Convert label to numeric for classification using boosting
credit_train$PROFITABLE = as.numeric(credit_train$PROFITABLE)
credit_valid$PROFITABLE = as.numeric(credit_valid$PROFITABLE)
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
credit_train$PROFITABLE
credit_dataset$PROFITABLE
credit_test$PROFITABLE
as.integer(credit_test$PROFITABLE)
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
library(randomForest)
library(gbm)
#set seed
set.seed(12345)
#load data
setwd("C:\\Users\\vivek_000\\Desktop\\Data Mining\\Assignment-3")
credit_dataset <-read.csv("Credit_Dataset.csv")
#create labels
credit_dataset$PROFITABLE = ifelse(credit_dataset$PROFIT>=0,1,0)
#convert required variables into factors
credit_dataset$CHK_ACCT = as.factor(credit_dataset$CHK_ACCT)
credit_dataset$SAV_ACCT = as.factor(credit_dataset$SAV_ACCT)
credit_dataset$HISTORY = as.factor(credit_dataset$HISTORY)
credit_dataset$JOB = as.factor(credit_dataset$JOB)
credit_dataset$TYPE = as.factor(credit_dataset$TYPE)
#Convert label to factor for classification using bagging & random forest
credit_dataset$PROFITABLE = as.factor(credit_dataset$PROFITABLE)
#split test/ rest
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
credit_test = credit_dataset[test_inst,]
credit_rest = credit_dataset[-test_inst,]
#split valid/ train
valid_inst = sample(nrow(credit_rest),0.25*nrow(credit_rest))
credit_valid = credit_rest[valid_inst,]
credit_train = credit_rest[-valid_inst,]
train_inst = sample(nrow(credit_train),nrow(credit_train)) #get the train instances for our subset
#a)
#build bagging model on train data with default settings - 500 bags
bag_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=10,importance=TRUE)
bag_model
pred1 = predict(bag_model, newdata = credit_valid)
pred1
#compute valid accuracy
valid_len = nrow(credit_valid) #get len of valid
bag_acc <- sum(ifelse(pred1==credit_valid$PROFITABLE,1,0))/valid_len
bag_acc
#b)
#build random forest model on train data with default settings - 500 trees
#mtry = sqrt(10)~3
rf_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=3,importance=TRUE)
rf_model
pred2 = predict(rf_model, newdata = credit_valid)
pred2
#compute valid accuracy
rf_acc <- sum(ifelse(pred2==credit_valid$PROFITABLE,1,0))/valid_len
rf_acc
#Convert label to numeric for classification using boosting
credit_train$PROFITABLE = ifelse(credit_train$PROFITABLE==0,0,1)
credit_train$PROFITABLE
credit_valid$PROFITABLE = ifelse(credit_valid$PROFITABLE==0,0,1)
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
boost_model
pred3_prob = predict(boost_model, newdata = credit_valid)
pred3_prob
pred3_prob = predict(boost_model, newdata = credit_valid, n.trees = 500)
pred3_prob
pred3 = ifelse(pred3_prob>0.5,1,0)
pred3
#compute valid accuracy
boost_acc <- sum(ifelse(pred3==credit_valid$PROFITABLE,1,0))/valid_len
boost_acc
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 1000, interaction.depth = 4)
boost_model
pred3_prob = predict(boost_model, newdata = credit_valid, n.trees = 500)
pred3_prob
pred3 = ifelse(pred3_prob>0.5,1,0)
pred3
#compute valid accuracy
boost_acc <- sum(ifelse(pred3==credit_valid$PROFITABLE,1,0))/valid_len
boost_acc
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
boost_model
pred3_prob = predict(boost_model, newdata = credit_valid, n.trees = 500)
pred3_prob
pred3 = ifelse(pred3_prob>0.5,1,0)
pred3
#compute valid accuracy
boost_acc <- sum(ifelse(pred3==credit_valid$PROFITABLE,1,0))/valid_len
boost_acc
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
boost_model
pred3_prob = predict(boost_model, newdata = credit_valid, n.trees = 500)
pred3_prob
pred3 = ifelse(pred3_prob>0.5,1,0)
pred3
#compute valid accuracy
boost_acc <- sum(ifelse(pred3==credit_valid$PROFITABLE,1,0))/valid_len
boost_acc
credit_test$PROFITABLE
#compute test length
test_len = nrow(credit_test)
test_len
View(credit_test)
#------Bagging
#predict on test
pred1_test = predict(bag_model, newdata = credit_test)
pred1_test
#compute test accuracy
bag_acc_test <- sum(ifelse(pred1_test==credit_test$PROFITABLE,1,0))/test_len
bag_acc_test
#------Random Forest
#predict on test
pred2_test = predict(rf_model, newdata = credit_test)
pred2_test
#compute test accuracy
rf_acc_test <- sum(ifelse(pred2_test==credit_test$PROFITABLE,1,0))/test_len
rf_acc_test
#------Boosting
#Convert label to numeric for classification using boosting
credit_test$PROFITABLE = ifelse(credit_test$PROFITABLE==0,0,1)
credit_test$PROFITABLE
#predict on test
pred3_prob_test = predict(boost_model, newdata = credit_test, n.trees = 500)
pred3_prob_test
pred3_test = ifelse(pred3_prob_test>0.5,1,0)
pred3_test
#compute test accuracy
boost_acc_test <- sum(ifelse(pred3==credit_test$PROFITABLE,1,0))/test_len
#compute test accuracy
boost_acc_test <- sum(ifelse(pred3_test==credit_test$PROFITABLE,1,0))/test_len
boost_acc_test
bag_acc
rf_acc
boost_acc
bag_acc_test
rf_acc_test
boost_acc_test
pred3_prob
pred3_prob_test
library(randomForest)
library(gbm)
#set seed
set.seed(12345)
#load data
setwd("C:\\Users\\vivek_000\\Desktop\\Data Mining\\Assignment-3")
credit_dataset <-read.csv("Credit_Dataset.csv")
credit_dataset$PROFITABLE = ifelse(credit_dataset$PROFIT>=0,1,0)
#convert required variables into factors
credit_dataset$CHK_ACCT = as.factor(credit_dataset$CHK_ACCT)
credit_dataset$SAV_ACCT = as.factor(credit_dataset$SAV_ACCT)
credit_dataset$HISTORY = as.factor(credit_dataset$HISTORY)
credit_dataset$JOB = as.factor(credit_dataset$JOB)
credit_dataset$TYPE = as.factor(credit_dataset$TYPE)
credit_dataset$PROFITABLE = as.factor(credit_dataset$PROFITABLE)
#split test/ rest
test_inst = sample(nrow(credit_dataset),0.3*nrow(credit_dataset))
credit_test = credit_dataset[test_inst,]
credit_rest = credit_dataset[-test_inst,]
#split valid/ train
valid_inst = sample(nrow(credit_rest),0.25*nrow(credit_rest))
credit_valid = credit_rest[valid_inst,]
credit_train = credit_rest[-valid_inst,]
train_inst = sample(nrow(credit_train),nrow(credit_train))
#a)
#build bagging model on train data with default settings - 500 bags
bag_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=10,importance=TRUE)
bag_model
#predict on valid
pred1 = predict(bag_model, newdata = credit_valid)
pred1
#compute valid accuracy
valid_len = nrow(credit_valid) #get len of valid
bag_acc <- sum(ifelse(pred1==credit_valid$PROFITABLE,1,0))/valid_len
bag_acc
#b)
#build random forest model on train data with default settings - 500 trees
#mtry = sqrt(10)~3
rf_model=randomForest(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_dataset,subset =train_inst,mtry=3,importance=TRUE)
rf_model
#predict on valid
pred2 = predict(rf_model, newdata = credit_valid)
pred2
#compute valid accuracy
rf_acc <- sum(ifelse(pred2==credit_valid$PROFITABLE,1,0))/valid_len
rf_acc
#c)
#build boost model on train data with custom settings - 500 trees, interaction depth = 4
#Convert label to numeric for classification using boosting
credit_train$PROFITABLE = ifelse(credit_train$PROFITABLE==0,0,1)
credit_valid$PROFITABLE = ifelse(credit_valid$PROFITABLE==0,0,1)
boost_model=gbm(PROFITABLE~AGE+DURATION+RENT+TELEPHONE+FOREIGN+CHK_ACCT+SAV_ACCT+HISTORY+JOB+TYPE,data = credit_train, distribution = "bernoulli", n.trees = 500, interaction.depth = 4)
boost_model
#predict on valid
pred3_prob = predict(boost_model, newdata = credit_valid, n.trees = 500)
pred3_prob
pred3 = ifelse(pred3_prob>0.5,1,0)
pred3
#compute valid accuracy
boost_acc <- sum(ifelse(pred3==credit_valid$PROFITABLE,1,0))/valid_len
boost_acc
#d)compute accuracy on test to select best model
#compute test length
test_len = nrow(credit_test)
test_len
#------Bagging
#predict on test
pred1_test = predict(bag_model, newdata = credit_test)
pred1_test
#compute test accuracy
bag_acc_test <- sum(ifelse(pred1_test==credit_test$PROFITABLE,1,0))/test_len
bag_acc_test
#------Random Forest
#predict on test
pred2_test = predict(rf_model, newdata = credit_test)
pred2_test
#compute test accuracy
rf_acc_test <- sum(ifelse(pred2_test==credit_test$PROFITABLE,1,0))/test_len
rf_acc_test
#------Boosting
#Convert label to numeric for classification using boosting
credit_test$PROFITABLE = ifelse(credit_test$PROFITABLE==0,0,1)
#predict on test
pred3_prob_test = predict(boost_model, newdata = credit_test, n.trees = 500)
pred3_prob_test
pred3_test = ifelse(pred3_prob_test>0.5,1,0)
pred3_test
#compute test accuracy
boost_acc_test <- sum(ifelse(pred3_test==credit_test$PROFITABLE,1,0))/test_len
boost_acc_test
bag_acc
rf_acc
bag_acc_test
rf_acc_test
boost_acc
boost_acc_test
#test to compute accuracy from confusion matrix
pred4 = predict(boost_model, credit_train)
pred4
cm=table(credit_train$PROFITABLE, pred4)
cm
#test to compute accuracy from confusion matrix
pred4 = predict(boost_model, credit_train, n.trees = 500)
pred4
cm=table(credit_train$PROFITABLE, pred4)
cm
#test to compute accuracy from confusion matrix
pred4 = predict(boost_model, credit_train, n.trees = 500)
pred4
pred4_c = ifelse(pred4_c>0.5,1,0)
pred4_c
#test to compute accuracy from confusion matrix
pred4 = predict(boost_model, credit_train, n.trees = 500)
pred4
pred4_c = ifelse(pred4>0.5,1,0)
pred4_c
cm=table(credit_train$PROFITABLE, pred4_C)
cm=table(credit_train$PROFITABLE, pred4_c)
cm
